{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f8cf02-0787-4558-979c-52d19d685a64",
   "metadata": {},
   "source": [
    "Importing libraries and cleaning up the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "981692f2-6745-4060-909e-f710496f2c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pathlib import Path  \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "dem_candidates = pd.read_csv('../datasets/dem_candidates.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1349af56-a806-4b0b-947c-124f8b6edda7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Sara -- Frequentist Model using statsmodels.api\n",
    "Why: From lab 5:\n",
    "Let's start by considering the problem from a frequentist lens. To do this, we'll use the statsmodels.api, which allows us to create a model in just a few lines of code.\n",
    "\n",
    "After fitting our model, we can call the .summary() method, and get a breakdown of our model and some details on how well it fit our data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c776f91d-49a3-4888-bf2f-b0c723282c43",
   "metadata": {},
   "source": [
    "2a) Fit Poisson GLM model where Temp_Anomaly is a covariate (exogenous variable): No need to modify\n",
    "freq_model = sm.GLM(df[\"Num_Storms\"], exog = sm.add_constant(df[\"Temp_Anomaly\"]), \n",
    "                  family=sm.families.Poisson())\n",
    "freq_res = freq_model.fit()\n",
    "print(freq_res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7a5a7d2a-2c0e-46d3-830e-2ee3b65a89b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:         General Status   No. Observations:                  633\n",
      "Model:                            GLM   Df Residuals:                      623\n",
      "Model Family:                Binomial   Df Model:                            9\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -335.48\n",
      "Date:                Wed, 06 Dec 2023   Deviance:                       670.95\n",
      "Time:                        23:49:03   Pearson chi2:                     622.\n",
      "No. Iterations:                     5   Pseudo R-squ. (CS):            0.09850\n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const                 0.2869      0.334      0.859      0.390      -0.368       0.941\n",
      "Partisan Lean        -0.0070      0.006     -1.241      0.215      -0.018       0.004\n",
      "Race                  0.0243      0.235      0.103      0.918      -0.437       0.486\n",
      "Veteran?             -0.0363      0.260     -0.140      0.889      -0.545       0.473\n",
      "LGBTQ?                0.2103      0.442      0.476      0.634      -0.656       1.077\n",
      "Self-Funder?          0.7353      0.490      1.502      0.133      -0.224       1.695\n",
      "STEM?                -0.5217      0.269     -1.938      0.053      -1.049       0.006\n",
      "Obama Alum?           0.9608      0.440      2.182      0.029       0.098       1.824\n",
      "Elected Official?     1.0651      0.306      3.484      0.000       0.466       1.664\n",
      "total_runners        -0.3619      0.064     -5.645      0.000      -0.488      -0.236\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "predictors = [ 'Partisan Lean', 'Race', 'Veteran?', 'LGBTQ?', 'Self-Funder?', 'STEM?','Obama Alum?', 'Elected Official?','total_runners']\n",
    "# Fit Poisson GLM model where Temp_Anomaly is a covariate (exogenous variable): No need to modify\n",
    "freq_model = sm.GLM(endog = house[\"General Status\"], exog = sm.add_constant(house[predictors]), \n",
    "                  family=sm.families.Binomial())\n",
    "freq_res = freq_model.fit()\n",
    "print(freq_res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd584534-b91b-47a3-8fb6-e50dad00e082",
   "metadata": {},
   "source": [
    "using predictors that have no relationship with the response tends to cause a deterioration in the test error rate (since such predictors cause an increase in variance without a corresponding decrease in bias), and so removing such predictors may in turn yield an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b38fc-1fb0-4bf5-b027-d68953207db3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f56adcc6-8a01-4cdc-8dc2-5bbde6c4f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = (Smarket.Year < 2005)\n",
    "# Smarket_train = Smarket.loc[train]\n",
    "# Smarket_test = Smarket.loc[∼train]\n",
    "\n",
    "# X_train, X_test = X.loc[train], X.loc[∼train]\n",
    "# y_train, y_test = y.loc[train], y.loc[∼train]\n",
    "# glm_train = sm.GLM(\n",
    "#     y_train,\n",
    "#     X_train,\n",
    "#     family=sm.families.Binomial()\n",
    "# )\n",
    "# results = glm_train.fit()\n",
    "# probs = results.predict(exog=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459299ee-8b4a-4c9b-96f8-10216c9bab53",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fitting a Frequentist Logistic Regression model using Sklearn:\n",
    "To choose the predictors (X) we used domain knowledge. \n",
    "- X: Predictors of General Status\n",
    "- Y: General Status (1 if the candidate advanced to the general election, 0 if not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "81a02c96-bb75-4104-a971-45b959741c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "#setting up the logistic regression model by splitting into train and test\n",
    "train, test = train_test_split(house, test_size = .30, random_state = 101)\n",
    "predictors = [ 'Partisan Lean', 'Race', 'Veteran?', 'LGBTQ?', 'Self-Funder?', 'STEM?','Obama Alum?', 'total_runners']\n",
    "X_train, y_train = train[predictors], train['General Status']\n",
    "X_test, y_test = test[predictors], test['General Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1fb8f43c-3f9a-4a8e-a60e-ac86ea0d30f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.7631578947368421\n"
     ]
    }
   ],
   "source": [
    "# X_train = train[['Partisan Lean', 'Race',\n",
    "#        'Veteran?', 'LGBTQ?', 'Elected Official?', 'Self-Funder?', 'STEM?',\n",
    "#        'Obama Alum?', 'Party Support?', 'Emily Endorsed?', 'total_runners']]\n",
    "# y_train = train['General Status']\n",
    "\n",
    "# X_test = test[['Partisan Lean', 'Race',\n",
    "#        'Veteran?', 'LGBTQ?', 'Elected Official?', 'Self-Funder?', 'STEM?',\n",
    "#        'Obama Alum?', 'Party Support?', 'Emily Endorsed?', 'total_runners']]\n",
    "# y_test = test['General Status']\n",
    "\n",
    "logisticmodel = LogisticRegression(penalty='none', solver='lbfgs')\n",
    "\n",
    "logisticmodel.fit(X_test, y_test)\n",
    "\n",
    "probs = logisticmodel.predict_proba(X_test)[:, 1]\n",
    "y_hat = (probs > 0.5).astype(int)\n",
    "\n",
    "accuracy = np.mean(y_test == y_hat)\n",
    "print(f\"Accuracy on test set: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62813175-e530-4638-a44c-8e4413f9e18f",
   "metadata": {},
   "source": [
    "We first created a Logistic Regression model using a subset of features in the house dataset. We employed feature engineering to include a column of how many total runners were participating in the election. Because the data on each candidate is not independent since the outcome of a race for a single candidate is affected by the outcome for another candidate in that same race, we included the number of total runners. Additionally, because both Biden and Warren only endorsed 5 candidates each, we excluded those features. Emily's list endorsed 42 candidates in the 2018 house of representative elections, so we intentionally included this column. There were 33 Obama Alums, so we included this as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2de59a76-7afd-4618-9d06-062cf7457295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house['Obama Alum?'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd19493-1750-4e03-9088-ff67aa5d879d",
   "metadata": {},
   "source": [
    "## Fitting a Logistic Regression model using statsmodel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a9d1c8bc-e397-4bee-81b6-3d066c5da0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.529978\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:         General Status   No. Observations:                  633\n",
      "Model:                          Logit   Df Residuals:                      623\n",
      "Method:                           MLE   Df Model:                            9\n",
      "Date:                Wed, 06 Dec 2023   Pseudo R-squ.:                 0.08911\n",
      "Time:                        23:49:03   Log-Likelihood:                -335.48\n",
      "converged:                       True   LL-Null:                       -368.29\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.084e-10\n",
      "=====================================================================================\n",
      "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const                 0.2869      0.334      0.859      0.390      -0.368       0.941\n",
      "Partisan Lean        -0.0070      0.006     -1.241      0.215      -0.018       0.004\n",
      "Race                  0.0243      0.235      0.103      0.918      -0.437       0.486\n",
      "Veteran?             -0.0363      0.260     -0.140      0.889      -0.545       0.473\n",
      "LGBTQ?                0.2103      0.442      0.476      0.634      -0.656       1.077\n",
      "Self-Funder?          0.7353      0.490      1.502      0.133      -0.224       1.695\n",
      "STEM?                -0.5217      0.269     -1.938      0.053      -1.049       0.006\n",
      "Obama Alum?           0.9608      0.440      2.182      0.029       0.098       1.824\n",
      "Elected Official?     1.0651      0.306      3.484      0.000       0.466       1.664\n",
      "total_runners        -0.3619      0.064     -5.645      0.000      -0.488      -0.236\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "predictors = [ 'Partisan Lean', 'Race', 'Veteran?', 'LGBTQ?', 'Self-Funder?', 'STEM?','Obama Alum?', 'Elected Official?','total_runners']\n",
    "X = house[predictors]  # independent variables\n",
    "\n",
    "# Add a constant term to the independent variables\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "y = house['General Status']\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = sm.Logit(y, X)\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f710cdb3-918c-4501-b777-1a6178209131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690.9515452801878 735.4562495016296\n"
     ]
    }
   ],
   "source": [
    "print(result.aic, result.bic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab702174-35d5-4e99-bfbe-d88506136893",
   "metadata": {},
   "source": [
    "While our initial model had a decent accuracy of approximately 84%, we can further optimize our model by selecting the best combination of features. As we can see in the summary from the model above, the log-likelihood is pretty low. Additionally, when we interpret the confidence intervals for the probabilities of the coefficients, we can see that many of these intervals include 0. Thus, it would make sense to re-evaluate these features or take them out. Additionally, when comparing the coefficients, we can see that some of the coefficients only have a marginal affect on the dependent variable, such as 'LGBTQ?' or 'Self-Funder?'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d87af89-9071-490e-a107-f862dcb19561",
   "metadata": {},
   "source": [
    "Forward selection of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1ccbba2f-b2c5-4055-90eb-a4ada5372426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.574478\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.574083\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.574473\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.574478\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.574388\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.570005\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.566736\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.558117\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.531759\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.531759\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:         General Status   No. Observations:                  633\n",
      "Model:                          Logit   Df Residuals:                      627\n",
      "Method:                           MLE   Df Model:                            5\n",
      "Date:                Wed, 06 Dec 2023   Pseudo R-squ.:                 0.08605\n",
      "Time:                        23:49:04   Log-Likelihood:                -336.60\n",
      "converged:                       True   LL-Null:                       -368.29\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.426e-12\n",
      "=====================================================================================\n",
      "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const                 0.2938      0.319      0.922      0.356      -0.331       0.918\n",
      "Partisan Lean        -0.0060      0.005     -1.100      0.271      -0.017       0.005\n",
      "STEM?                -0.4862      0.267     -1.819      0.069      -1.010       0.038\n",
      "Obama Alum?           0.9746      0.434      2.244      0.025       0.123       1.826\n",
      "Elected Official?     1.0181      0.302      3.374      0.001       0.427       1.610\n",
      "total_runners        -0.3501      0.063     -5.553      0.000      -0.474      -0.227\n",
      "===================================================================================== 685.2066955439864\n"
     ]
    }
   ],
   "source": [
    "predictors = [ 'Partisan Lean', 'Race', 'Veteran?', 'LGBTQ?', 'Self-Funder?', 'STEM?','Obama Alum?', 'Elected Official?','total_runners']\n",
    "all_predictors_df = house[predictors]\n",
    "\n",
    "\n",
    "y = house['General Status']\n",
    "\n",
    "best_features = []\n",
    "best_aic = float('inf') \n",
    "\n",
    "for feature in all_predictors_df.columns:\n",
    "    # Add a constant term and the current feature\n",
    "    X = sm.add_constant(all_predictors_df[best_features + [feature]])\n",
    "\n",
    "    # Fit the logistic regression model\n",
    "    model = sm.Logit(y, X)\n",
    "    result = model.fit()\n",
    "\n",
    "    # Check AIC and update if it is lower\n",
    "    if result.aic < best_aic:\n",
    "        best_aic = result.aic\n",
    "        best_features.append(feature)\n",
    "\n",
    "# Fit the final model with the best features\n",
    "X_final = sm.add_constant(all_predictors_df[best_features])\n",
    "final_model = sm.Logit(y, X_final)\n",
    "final_result = final_model.fit()\n",
    "\n",
    "\n",
    "print(final_result.summary(), final_result.aic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d88b3f-cf09-4cd7-b123-f5fc59aa40ea",
   "metadata": {},
   "source": [
    "@ Nikki I am not sure what this is for? \n",
    "train, test = train_test_split(all_predictors_df, test_size = .30, random_state = 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3e62f-da59-4870-920f-c2e8b5a9719f",
   "metadata": {},
   "source": [
    "Backward selection of features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "da208092-485e-4280-a2c5-d3f709d6344f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.529978\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.539299\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.531212\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.532558\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.535826\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.535916\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.532589\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.571044\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.532723\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.532730\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.535342\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.532730\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:         General Status   No. Observations:                  633\n",
      "Model:                          Logit   Df Residuals:                      628\n",
      "Method:                           MLE   Df Model:                            4\n",
      "Date:                Wed, 06 Dec 2023   Pseudo R-squ.:                 0.08438\n",
      "Time:                        23:49:04   Log-Likelihood:                -337.22\n",
      "converged:                       True   LL-Null:                       -368.29\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.023e-12\n",
      "=====================================================================================\n",
      "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "Elected Official?     0.9345      0.291      3.209      0.001       0.364       1.505\n",
      "const                 0.5137      0.248      2.074      0.038       0.028       0.999\n",
      "Obama Alum?           0.9038      0.429      2.105      0.035       0.062       1.745\n",
      "total_runners        -0.3775      0.058     -6.507      0.000      -0.491      -0.264\n",
      "STEM?                -0.4716      0.267     -1.769      0.077      -0.994       0.051\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "predictors = [ 'Partisan Lean', 'Race', 'Veteran?', 'LGBTQ?', 'Self-Funder?', 'STEM?','Obama Alum?', 'Elected Official?','total_runners']\n",
    "all_predictors_df = house[predictors]\n",
    "y = house['General Status']\n",
    "\n",
    "X = sm.add_constant(all_predictors_df)  #.drop returns a df\n",
    "X_shuffled = X.sample(frac=1, axis=1, random_state=42)\n",
    "\n",
    "\n",
    "# Fit the initial model with all features\n",
    "initial_model = sm.Logit(y, X_shuffled)\n",
    "initial_result = initial_model.fit()\n",
    "best_aic = initial_result.aic\n",
    "best_bic = initial_result.bic\n",
    "best_model = initial_model\n",
    "selected_features = list(X_shuffled.columns)\n",
    "\n",
    "# Backward selection\n",
    "\n",
    "# @ Nikki why do we drop the constant\n",
    "for feature in X_shuffled.columns[0:]:  # Exclude the constant term\n",
    "    # Fit the model without the current feature\n",
    "    X_subset = X_shuffled[selected_features].drop(feature, axis=1)\n",
    "    model = sm.Logit(y, X_subset)\n",
    "    result = model.fit()\n",
    "\n",
    "    # Compare AIC and BIC\n",
    "    if result.aic < best_aic and result.bic < best_bic:\n",
    "        best_aic = result.aic\n",
    "        best_bic = result.bic\n",
    "        best_model = model\n",
    "        selected_features.remove(feature)\n",
    "\n",
    "# Fit the final model with the selected features\n",
    "final_result = best_model.fit()\n",
    "\n",
    "# Print the summary of the final model\n",
    "print(final_result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44ea1a7-23d5-4c72-b3d9-e10de1454962",
   "metadata": {},
   "source": [
    "Testing the best recommended features in a logistic regression model with sklearn: \n",
    "We will only take the four most signifcant regressors according to the forward and backwrard seelction modles/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "18f9468b-c6a7-4482-a8bd-d245187346db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.7368421052631579\n"
     ]
    }
   ],
   "source": [
    "#house_important_only = house[house['Elected Official','STEM?','Obama Alum?', 'total_runners', 'General Status']]\n",
    "train, test = train_test_split(house, test_size = .30, random_state = 101)\n",
    "new_predictors = ['Elected Official?','STEM?','Obama Alum?', 'total_runners']\n",
    "X_train1, y_train1 = train[new_predictors], train['General Status']\n",
    "X_test1, y_test1 = test[new_predictors], test['General Status']\n",
    "\n",
    "logisticmodel1 = LogisticRegression(\n",
    "    penalty='none', solver='lbfgs'\n",
    ")\n",
    "\n",
    "logisticmodel1.fit(X_test1, y_test1)\n",
    "\n",
    "probs = logisticmodel1.predict_proba(X_test1)[:, 1]\n",
    "y_hat = (probs > 0.5).astype(np.int64)\n",
    "\n",
    "accuracy = np.mean(y_test == y_hat)\n",
    "print(f\"Accuracy on test set: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf613b5c-3afc-4f89-a600-2eb775254174",
   "metadata": {},
   "source": [
    "Nonparametric models: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "55dd6514-c79d-4f7e-b49b-7a8b7597ddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Using the optimal features selected in the previous section: -- dominic said to do all possible ones. \n",
    "\n",
    "X_cols = [ 'Partisan Lean', 'Race', 'Veteran?', 'LGBTQ?', 'Self-Funder?', 'STEM?','Obama Alum?', 'Elected Official?','total_runners']\n",
    "y_col = 'General Status'\n",
    "forest_model = RandomForestClassifier()\n",
    "\n",
    "train, test = train_test_split(house, test_size = .25, random_state = 101)\n",
    "forest_model.fit(train[X_cols], train['General Status'])\n",
    "\n",
    "train[\"forest_pred\"] = forest_model.predict(train[X_cols])\n",
    "test[\"forest_pred\"] = forest_model.predict(test[X_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8542264d-27cb-4b51-8719-85a069429d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set error for random forest: 0.27939053622117876\n",
      "Test set error for random forest:     0.6193930890332394\n",
      "Accuracy: 0.6163522012578616\n"
     ]
    }
   ],
   "source": [
    "train_rmse = np.mean((train[\"forest_pred\"] - train[y_col]) ** 2) ** 0.5\n",
    "test_rmse = np.mean((test[\"forest_pred\"] - test[y_col]) ** 2) ** 0.5\n",
    "forest_accuracy = accuracy_score(test[y_col], test[\"forest_pred\"])\n",
    "\n",
    "print(\"Training set error for random forest:\", train_rmse)\n",
    "print(\"Test set error for random forest:    \", test_rmse)\n",
    "print(\"Accuracy:\", forest_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2ba892-6bbb-4f39-bc52-dd4302d7470b",
   "metadata": {},
   "source": [
    "Nonparametric models: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6f7f8747-4726-44bf-a266-4dfb350c4056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "tree_model = DecisionTreeClassifier()\n",
    "\n",
    "tree_model.fit(train[X_cols], train[y_col])\n",
    "\n",
    "train[\"tree_pred\"] = tree_model.predict(train[X_cols])\n",
    "test[\"tree_pred\"] = tree_model.predict(test[X_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ba12afb8-4740-48ed-ad4c-676ef83b11f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set error for decision tree: 0.27939053622117876\n",
      "Test set error for decision tree:     0.6344412685745153\n",
      "Accuracy: 0.5974842767295597\n"
     ]
    }
   ],
   "source": [
    "train_rmse = np.mean((train[\"tree_pred\"] - train[y_col]) ** 2) ** 0.5\n",
    "test_rmse = np.mean((test[\"tree_pred\"] - test[y_col]) ** 2) ** 0.5\n",
    "\n",
    "print(\"Training set error for decision tree:\", train_rmse)\n",
    "print(\"Test set error for decision tree:    \", test_rmse)\n",
    "\n",
    "\n",
    "tree_accuracy = accuracy_score(test[y_col], test[\"tree_pred\"])\n",
    "print(\"Accuracy:\", tree_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c386dfe0-e80b-4057-b88d-e2e4e9ccf440",
   "metadata": {},
   "source": [
    "```\n",
    "Evaluation Metrics:\n",
    "\n",
    "Besides RMSE and accuracy, consider other metrics like precision, recall, F1 score, especially for imbalanced datasets.\n",
    "\n",
    "Model Complexity:\n",
    "\n",
    "Random Forest's max_features=1 might be too restrictive.\n",
    "Try adjusting max_features and other hyperparameters like n_estimators and max_depth.\n",
    "Model Overfitting:\n",
    "\n",
    "Both models might be overfitting to the training data.\n",
    "Consider implementing cross-validation to better understand model performance.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56fa09b-c869-42ea-87c2-83759601e779",
   "metadata": {},
   "source": [
    "## Looking into Prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "69972750-b009-4eb1-83d4-1aa0ae8c4fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prevalence of wins: 0.2685624012638231\n"
     ]
    }
   ],
   "source": [
    "#number of rows in house to beign with that actually won. \n",
    "prevalence = len(house[house['General Status']==1])/len(house)\n",
    "print(\"Prevalence of wins:\", prevalence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37df85f-de2e-46c4-a6f4-84898e5ad6bb",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 2b) Frequentist Regression\n",
    "\n",
    "Let's start by considering the problem from a frequentist lens. To do this, we'll use the `statsmodels.api`, which allows us to create a model in just a few lines of code.\n",
    "\n",
    "After fitting our model, we can call the `.summary()` method, and get a breakdown of our model and some details on how well it fit our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb338cb5-9844-4b09-9acc-d51e05f02e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0af799ef-629f-48a6-8dfd-2264658187b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#FROM LAB \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Fit Poisson GLM model where Temp_Anomaly is a covariate (exogenous variable): No need to modify\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m freq_model \u001b[38;5;241m=\u001b[39m \u001b[43msm\u001b[49m\u001b[38;5;241m.\u001b[39mGLM(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNum_Storms\u001b[39m\u001b[38;5;124m\"\u001b[39m], exog \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39madd_constant(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTemp_Anomaly\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \n\u001b[1;32m      4\u001b[0m                   family\u001b[38;5;241m=\u001b[39msm\u001b[38;5;241m.\u001b[39mfamilies\u001b[38;5;241m.\u001b[39mPoisson())\n\u001b[1;32m      5\u001b[0m freq_res \u001b[38;5;241m=\u001b[39m freq_model\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(freq_res\u001b[38;5;241m.\u001b[39msummary())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sm' is not defined"
     ]
    }
   ],
   "source": [
    "#FROM LAB \n",
    "# Fit Poisson GLM model where Temp_Anomaly is a covariate (exogenous variable): No need to modify\n",
    "freq_model = sm.GLM(df[\"Num_Storms\"], exog = sm.add_constant(df[\"Temp_Anomaly\"]), \n",
    "                  family=sm.families.Poisson())\n",
    "freq_res = freq_model.fit()\n",
    "print(freq_res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2723faf-105c-44c4-8b5f-1fa2c1947232",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_two = sm.OLS(np.log(Y), sm.add_constant(X)).fit()\n",
    "print(model_two.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab8517d-2e33-47da-9b72-5f229f53f2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
